# Universal-Transformer-Pytorch
Simple and self-contained implementation of the Universal Transformer in Pytorch. Please open issues if you find bugs, and send pull request if you want to contribuite. 

## Universal Transformer 
The basic Transformer model has been taken from [https://github.com/kolloldas/torchnlp](https://github.com/kolloldas/torchnlp). For now has been implemented:

- Universal Encoder Decoder, with position and time embeddings.
- Universal Transformer for bAbI data.  

## How to run
```
python main.py --task 1
```

## TODO
- APT (Hopefully by this week will be ready)
- Modularize the code in different files
- Run task 1-20 and report performance
